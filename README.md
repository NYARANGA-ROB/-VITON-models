# preprocessing-for-VITON-models
## 1- Cloth mask
code uses a pre-trained segmentation model (U-net) and an input image.
It preprocesses the image for the model.
* Preprocessing the image: 
Pads the image to ensure its dimensions are divisible by 32, which is necessary for the models that we will use.
* Preparing Input for Model Inference: 
After setting up an albumentations transformation pipeline for normalization, we apply the normalization transformation to the padded image, then convert the normalized image to a PyTorch tensor and add a batch dimension.
Performs inference using the pre-trained model to obtain a segmentation mask.
* Postprocessing the segmentation mask: It converts the model's output to a binary mask, removes padding from the mask, and then resizes the masked image.
<p align="middle">     
    <img src="https://github.com/Aalaa4444/preprocessing-for-VITON-models/blob/main/cloth_mask/cloth.jpg" width="200">     
    <img src="https://github.com/Aalaa4444/preprocessing-for-VITON-models/blob/main/cloth_mask/cloth-mask.jpg" width="200">    
</p>

## 2- image-parse-v3
It is a detailed semantic parsing of an image to identify and segment different human body parts, distinguishing between different regions like the head, torso, arms, and legs.
* Images are loaded and preprocessed (e.g., resized, normalized, and sometimes augmented to ensure consistency) to match the model's requirements. The segmentation models (U-Net, DeeplabV3+, and HRNet) are then given the preprocessed images. These models provide segmented masks that represent various body parts, including the head, chest, arms, legs, and occasionally finer features like hands and feet. The segmented masks are refined and used to guide the overlay of clothing items in the VITON-HD framework.
<p align="middle">     
    <img src="https://github.com/Aalaa4444/preprocessing-for-VITON-models/blob/main/test_img.jpg" width="200">     
    <img src="https://github.com/Aalaa4444/preprocessing-for-VITON-models/blob/main/image_parse/densepose.png" width="200">    
</p>

## 3- Openpose
using human pose estimation models (OpenPose 1.17), This preprocessing step involves detecting key points on the human body to accurately map the clothing onto the person.
OpenPose is open-source tool for real-time multi-person keypoint detection. It provides keypoints for the entire body, including the face, hands, and body joints. The key points detected by OpenPose serve as crucial inputs for generating precise clothing alignment in virtual try-on applications. 
* The keypoints detected by OpenPose are fundamental for:
  * Pose Estimation: Accurate detection of body posture and orientation.
  *  Clothing Alignment: Ensuring that clothing items are correctly placed and deformed according to the person's pose.
* Output of OpenPose 1.17
  *	JSON Files: contains detailed keypoint data for corresponding images. This data includes coordinates for body, face, and hand keypoints, which are critical for precise clothing alignment.
  * Images:  include visual representations of detected keypoints.
<p align="middle">     
    <img src="https://github.com/Aalaa4444/preprocessing-for-VITON-models/blob/main/test_img.jpg" width="200">     
    <img src="https://github.com/Aalaa4444/preprocessing-for-VITON-models/blob/main/openpose/openpose.png" width="200">    
</p>

## 4- DensePose
DensePose is a project created by Facebook AI Research (FAIR) that tries to map all human pixels in an RGB image to the 3D surface of the human body.
Unlike typical pose estimation approaches that predict key points, DensePose performs dense human body pose estimation, providing a pixel-to-surface correspondence. This allows for a more detailed and precise understanding of human poses in images.
Why use DensePose models? Detailed human pose estimation that produces dense correspondences rather than just key points, allowing for more detailed examination and analysis.
Robust performances were The models are trained on big datasets, ensuring consistent performance under a variety of conditions.
It can be used in a variety of disciplines, including augmented reality, virtual try-on, human-computer interaction, video analysis, gaming, AR/VR, and fashion.

The advantages of using DensePose are that it improves human understanding by improving our comprehension of human poses by mapping each pixel to a 3D body surface.
* Models used in DensePose : 
DensePose uses a variety of deep learning models to do its tasks. The main models use the ResNet architecture, specifically:
1. DensePose R-CNN: This model combines the DensePose pipeline with the region-based convolutional neural network architecture.
2. ResNet101 FPN (Feature Pyramid Network) is a feature extraction algorithm that balances speed and accuracy.
3. ResNet50 FPN is a lighter variant of ResNet101, resulting in faster inference times but reduced accuracy.
<p align="middle">     
    <img src="https://github.com/Aalaa4444/preprocessing-for-VITON-models/blob/main/densepose/densepose_output.png" width="200">     
    <img src="https://github.com/Aalaa4444/preprocessing-for-VITON-models/blob/main/densepose/image.png" width="200">    
</p>
